@article{LagaHamid2022ASoD,
abstract = {Estimating depth from RGB images is a long-standing ill-posed problem, which has been explored for decades by the computer vision, graphics, and machine learning communities. Among the existing techniques, stereo matching remains one of the most widely used in the literature due to its strong connection to the human binocular system. Traditionally, stereo-based depth estimation has been addressed through matching hand-crafted features across multiple images. Despite the extensive amount of research, these traditional techniques still suffer in the presence of highly textured areas, large uniform regions, and occlusions. Motivated by their growing success in solving various 2D and 3D vision problems, deep learning for stereo-based depth estimation has attracted a growing interest from the community, with more than 150 papers published in this area between 2014 and 2019. This new generation of methods has demonstrated a significant leap in performance, enabling applications such as autonomous driving and augmented reality. In this paper, we provide a comprehensive survey of this new and continuously growing field of research, summarize the most commonly used pipelines, and discuss their benefits and limitations. In retrospect of what has been achieved so far, we also conjecture what the future may hold for deep learning-based stereo for depth estimation research.},
author = {Laga, Hamid and Jospin, Laurent Valentin and Boussaid, Farid and Bennamoun, Mohammed},
address = {United States},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2022},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms ; Augmented reality ; Australia ; Computer vision ; Human beings ; Machine learning ; Pipelines ; Training},
language = {eng},
number = {4},
pages = {1738-1764},
publisher = {IEEE},
title = {A Survey on Deep Learning Techniques for Stereo-Based Depth Estimation},
volume = {44},
year = {2022},
}
 

@inproceedings{LeeVictorW2010Dt1G,
abstract = {Recent advances in computing have led to an explosion in the amount of data being generated. Processing the ever-growing data in a timely manner has made throughput computing an important aspect for emerging applications. Our analysis of a set of important throughput computing kernels shows that there is an ample amount of parallelism in these kernels which makes them suitable for today's multi-core CPUs and GPUs. In the past few years there have been many studies claiming GPUs deliver substantial speedups (between 10X and 1000X) over multi-core CPUs on these kernels. To understand where such large performance difference comes from, we perform a rigorous performance analysis and find that after applying optimizations appropriate for both CPUs and GPUs the performance gap between an Nvidia GTX280 processor and the Intel Core i7-960 processor narrows to only 2.5x on average. In this paper, we discuss optimization techniques for both CPU and GPU, analyze what architecture features contributed to performance differences between the two architectures, and recommend a set of architectural features which provide significant improvement in architectural efficiency for throughput kernels.},
author = {Lee, Victor W. and Kim, Changkyu and Chhugani, Jatin and Deisher, Michael and Kim, Daehyun and Nguyen, Anthony D. and Satish, Nadathur and Smelyanskiy, Mikhail and Chennupaty, Srinivas and Hammarlund, Per and Singhal, Ronak and Dubey, Pradeep},
address = {New York, NY, USA},
booktitle = {Proceedings of the 37th annual international symposium on Computer architecture},
copyright = {2010 ACM},
isbn = {9781450300537},
language = {eng},
pages = {451-460},
publisher = {ACM},
title = {Debunking the 100X GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU},
year = {2010},
}

@article{RorizRicardo2022ALTA,
abstract = {Nowadays, and more than a decade after the first steps towards autonomous driving, we keep heading to achieve fully autonomous vehicles on our roads, with LiDAR sensors being a key instrument for the success of this technology. Such advances trigger the emergence of new players in the automotive industry, and along with car manufacturers, this sector represents a multibillion-dollar market where everyone wants to take a share. To understand recent advances and technologies behind LiDAR, this article presents a survey on LiDAR sensors for the automotive industry. With this work, we show the measurement principles and imaging techniques currently being used, going through a review of commercial systems and development solutions available in the market today. Furthermore, we highlight the current and future challenges, providing insights on how both research and industry can step towards better LiDAR solutions.},
author = {Roriz, Ricardo and Cabral, Jorge and Gomes, Tiago},
address = {New York},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2022},
issn = {1524-9050},
journal = {IEEE transactions on intelligent transportation systems},
keywords = {Automated vehicles ; Automobiles ; Optical radar ; Three-dimensional imaging},
language = {eng},
number = {7},
pages = {6282-6297},
publisher = {IEEE},
title = {Automotive LiDAR Technology: A Survey},
volume = {23},
year = {2022},
}

@article{MaXinzhu20243ODF,
abstract = {3D object detection from images, one of the fundamental and challenging problems in autonomous driving, has received increasing attention from both industry and academia in recent years. Benefiting from the rapid development of deep learning technologies, image-based 3D detection has achieved remarkable progress. Particularly, more than 200 works have studied this problem from 2015 to 2021, encompassing a broad spectrum of theories, algorithms, and applications. However, to date no recent survey exists to collect and organize this knowledge. In this paper, we fill this gap in the literature and provide the first comprehensive survey of this novel and continuously growing research field, summarizing the most commonly used pipelines for image-based 3D detection and deeply analyzing each of their components. Additionally, we also propose two new taxonomies to organize the state-of-the-art methods into different categories, with the intent of providing a more systematic review of existing methods and facilitating fair comparisons with future works. In retrospect of what has been achieved so far, we also analyze the current challenges in the field and discuss future directions for image-based 3D detection research.},
author = {Ma, Xinzhu and Ouyang, Wanli and Simonelli, Andrea and Ricci, Elisa},
address = {United States},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2024},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms ; Classification ; Optical radar ; Surveys ; Task analysis},
language = {eng},
number = {5},
pages = {3537-3556},
publisher = {IEEE},
title = {3D Object Detection From Images for Autonomous Driving: A Survey},
volume = {46},
year = {2024},
}


@article{MengZeYu2024TODf,
abstract = {To ensure safe and efficient intelligent transportation systems (ITS), autonomous driving systems must have excellent abilities about object detection and environmental perception. Fusing data from multiple sensors can overcome inherent limitations of single-sensor perception in 3D object detection for autonomous driving. LiDAR is great at pinpointing objects but doesn't capture their velocity. Radar, on the other hand, accurately measures velocity but doesn't provide height details. Fusing Radar and LiDAR can extend the detection range and improve the detection performance for dynamic objects. Nevertheless, direct integration of two sensors for performance improvement is hindered by different data characteristics and noise distributions. To address this, we propose a novel fusion framework termed LiDAR and Pseudo 4D-Radar fusion under Bird's-Eye-View, dubbed L4R-BEVFusion, to overcome the challenge of multi-modal fusion. Specifically, Radar is a sensor that lacks object height information, we firstly design a pseudo 4D-Radar generation process that includes the sparse to dense(S2P) module and the height completion(RHC) module to transform the original 3D-Radar feature map into pseudo 4D-Radar feature map that is more dense and has enriched height information. Secondly, the fusion framework encodes the LiDAR features and the pseudo 4D-Radar features into the same Bird's-Eye-View(BEV) through cross-guided BEV encoder(CGBE) module. Extensive experiments show that L4R-BEVFusion achieves state-of-the-art performance (71.3% NDS and 66.7% mAP) for detecting dynamic objects(only use LiDAR and Radar) on the NuScenes dataset.},
author = {Meng, ZeYu and Song, YongHong and Zhang, YuanLin and Nan, YueXing and Bai, ZeNan},
issn = {1524-9050},
journal = {IEEE transactions on intelligent transportation systems},
keywords = {Optical radar ; Radar},
language = {eng},
number = {11},
pages = {18185-18195},
publisher = {IEEE},
title = {Traffic Object Detection for Autonomous Driving Fusing LiDAR and Pseudo 4D-Radar Under Bird's-Eye-View},
volume = {25},
year = {2024},
}

@inproceedings{SunJiaming2020DRS3,
abstract = {In this paper, we propose a novel system named Disp R-CNN for 3D object detection from stereo images. Many recent works solve this problem by first recovering a point cloud with disparity estimation and then apply a 3D detector. The disparity map is computed for the entire image, which is costly and fails to leverage category-specific prior. In contrast, we design an instance disparity estimation network (iDispNet) that predicts disparity only for pixels on objects of interest and learns a category-specific shape prior for more accurate disparity estimation. To address the challenge from scarcity of disparity annotation in training, we propose to use a statistical shape model to generate dense disparity pseudo-ground-truth without the need of LiDAR point clouds, which makes our system more widely applicable. Experiments on the KITTI dataset show that, even when LiDAR ground-truth is not available at training time, Disp R-CNN achieves competitive performance and outperforms previous state-of-the-art methods by 20% in terms of average precision. The code will be available at https://github.com/zju3dv/disprcnn.},
author = {Sun, Jiaming and Chen, Linghao and Xie, Yiming and Zhang, Siyu and Jiang, Qinhong and Zhou, Xiaowei and Bao, Hujun},
booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
isbn = {9781728171685},
issn = {2575-7075},
keywords = {Detectors ; Optical radar},
language = {eng},
pages = {10545-10554},
publisher = {IEEE},
title = {Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance Disparity Estimation},
year = {2020},
}


@article{hirschmuller2005babel,
  title={Accurate and Efficient Stereo Processing by Semi-Global Matching and Mutual Information.},
  author={Hirschmuller, H.},
  journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  volume={2},
  year={2005},
  langid={english}
}

@online{opencvsgbm,
title={cv::StereoSGBM},
version={3.4.20-dev},
url={https://docs.opencv.org/3.4/d2/d85/classcv_1_1StereoSGBM.html},
urldate={2024-03-17},
label={OpenCV StereoSGBM},
langid={english}
}


@online{pytorchfcnresnet50,
title={cv::ResNet},
version={3.4.20-dev},
url={https://pytorch.org/vision/main/models/generated/torchvision.models.segmentation.fcn_resnet50.html},
urldate={2024-03-17},
label={PyTorch fcn_resnet50},
langid={english}
}

@inproceedings{Cordts2016Cityscapes,
title={The Cityscapes Dataset for Semantic Urban Scene Understanding},
author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2016}
}

@book{alma9911523590705973,
abstract = {Book Contents - 1. Introduction to Machine Learning 2. Preparing to Model 3. Modelling and Evaluation 4. Basics of Feature Engineering 5. Brief Overview of Probability 6. Bayesian Concept Learning 7. Supervised Learning. Classification 8. Supervised Learning. Regression 9. Unsupervised Learning 10. Basics of Neural Network 11. Other Types of Learning Appendix A Programming Machine Learning in R Appendix B Programming Machine Learning in Python Appendix C A Case Study on Machine Learning Application. Grouping Similar Service Requests and Classifying a New One Model Question Paper-1 Model Question Paper-2 Model Question Paper-3.},
edition = {[First edition].},
author = {Dutt, Saikat and Chandramouli, Subramanian and Das, Amit Kuma},
address = {Uttar Pradesh, India},
booktitle = {Machine learning},
isbn = {93-5306-737-5},
keywords = {Machine learning},
language = {eng},
title = {Machine learning },
year = {2019},
publisher = {Pearson India},
}

@online{kagglecomvis,
title={Kaggle computer vision},
url={https://www.kaggle.com/datasets?tags=13207-Computer+Vision},
urldate={2018-12-27},
label={Kaggle computer vision},
langid={english}
}

@article{SumiYasushi20023ORi,
abstract = {We propose a new method for 3D object recognition which uses segment-based stereo vision. An object is identified in a cluttered environment and its position and orientation (6 dof) are determined accurately enabling a robot to pick up the object and manipulate it. The object can be of any shape (planar figures, polyhedra, free-form objects) and partially occluded by other objects. Segment-based stereo vision is employed for 3D sensing. Both CAD-based and sensor-based object modeling subsystems are available. Matching is performed by calculating candidates for the object position and orientation using local features, verifying each candidate, and improving the accuracy of the position and orientation by an iteration method. Several experimental results are presented to demonstrate the usefulness of the proposed method.[PUBLICATION ABSTRACT]},
author = {Sumi, Yasushi and Kawai, Yoshihiro and Yoshimi, Takashi and Tomita, Fumiaki},
address = {New York},
copyright = {Kluwer Academic Publishers 2002},
issn = {0920-5691},
journal = {International journal of computer vision},
language = {eng},
number = {1},
pages = {5-23},
title = {3D Object Recognition in Cluttered Environments by Segment-Based Stereo Vision},
volume = {46},
year = {2002},
publisher = {Springer Nature B.V},
}

@article{MemoryEfficientSGM,
  title={MEMORY EFFICIENT SEMI-GLOBAL MATCHING, ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., I-3, 371–376},
  author={Hirschmüller H. and Buder M. and Ernst I.},
  journal={https://doi.org/10.5194/isprsannals-I-3-371-2012},
  year={2012},
  langid={english},
}

@article{PhamTrungQuang2023EotH,
author = {Pham, Trung Quang and Matsui, Teppei and Chikazoe, Junichi},
address = {Basel},
copyright = {COPYRIGHT 2023 MDPI AG},
issn = {2079-7737},
journal = {Biology (Basel, Switzerland)},
keywords = {Attention ; Brain ; Computational linguistics ; Correspondence ; Letters ; Neural networks (Computer science) ; Neurons ; Neurosciences ; Review ; Reviews},
language = {eng},
number = {10},
pages = {1330-},
publisher = {MDPI AG},
title = {Evaluation of the Hierarchical Correspondence between the Human Brain and Artificial Neural Networks: A Review},
volume = {12},
year = {2023},
}

@article{NagalakshmiT2022BCSS,
author = {Nagalakshmi, T.},
address = {New York},
copyright = {The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022},
issn = {1370-4621},
journal = {Neural processing letters},
keywords = {Algorithms ; Artificial intelligence ; Automation ; Cluster analysis ; Computational intelligence ; Computer science ; Diagnostic imaging ; Efficiency ; Identification ; Image processing ; Machine learning ; Neural networks (Computer science) ; Semantics ; Support vector machines ; Tumors},
language = {eng},
number = {6},
pages = {5185-5198},
publisher = {Springer US},
title = {Breast Cancer Semantic Segmentation for Accurate Breast Cancer Detection with an Ensemble Deep Neural Network},
volume = {54},
year = {2022},
}


@article{ShiZhou2023VRBo,
author = {Shi, Zhou and Li, Zhongguo and Che, Sai and Gao, Miaowei and Tang, Hongchuan},
address = {Basel},
copyright = {2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2076-3417},
journal = {Applied sciences},
keywords = {Algorithms ; Cameras ; Drone aircraft ; Methodology ; Methods ; Research ; Telematics},
language = {eng},
number = {19},
pages = {10578-},
publisher = {MDPI AG},
title = {Visual Ranging Based on Object Detection Bounding Box Optimization},
volume = {13},
year = {2023},
}


@online{CrossEntropyLoss,
title={CrossEntropyLoss},
url={https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html},
urldate={2024-9-1},
label={CrossEntropyLoss},
langid={english}
}

@article{IkeuchiKatsushi1987DaDM,
author = {Ikeuchi, Katsushi},
address = {Thousand Oaks, CA},
copyright = {1988 INIST-CNRS},
issn = {0278-3649},
journal = {The International journal of robotics research},
keywords = {Artificial intelligence ; Image processing ; Robots ; Vision},
language = {eng},
number = {1},
pages = {15-31},
publisher = {Sage Publications},
title = {Determining a Depth Map Using a Dual Photometric Stereo},
volume = {6},
year = {1987},
}

@article{AnShiyong2021Asvs,
author = {An, Shiyong and Yang, Hongyu and Zhou, Pei and Xiao, Wenfan and Zhu, Jiangping and Guo, Yanqiong},
address = {United States},
copyright = {Copyright Optical Society of America Dec 10, 2021},
issn = {1559-128X},
journal = {Applied optics (2004)},
keywords = {Algorithms ; Calibration ; Cameras ; Diffraction patterns ; Image processing ; Image reconstruction ; Mathematical analysis ; Noise},
language = {eng},
number = {35},
pages = {10954-},
publisher = {Optical Society of America},
title = {Accurate stereo vision system calibration with chromatic concentric fringe patterns},
volume = {60},
year = {2021},
}


%%%%%%%%%%%%%%%%%%%% OLD 


@book{lamport1994latex,
  title={\LaTeX{}: a document preparation system: user's guide and reference manual},
  author={Lamport, Leslie},
  year={1994},
  publisher={Addison-Wesley},
  pagetotal={365},
  langid={english}
}

@article{braams1991babel,
  title={Babel, a multilingual style-option system for use with \LaTeX{}’s standard document styles},
  author={Braams, Johannes L},
  journal={TUGboat},
  volume={12},
  number={2},
  pages={291--301},
  year={1991},
  langid={english}
}

@manual{kirjoitusohje2018,
title={Tampereen yliopiston tekniikan alan opinnäytteiden kirjoitusohje},
organization={Tampereen yliopisto},
location={Tampere},
year={2018},
note={Saatavissa: POP $>$ Opiskelu $>$ Diplomityö $>$ Diplomityöohje},
label={Opinnäyteohje},
langid={finnish}
}

@manual{thesisguide2018,
title={Guide to Writing a Thesis in Technical Fields: Instructions for Master of Science and Bachelor of Science Theses},
organization={Tampere University},
location={Tampere},
year={2018},
note={Available: POP $>$ Study info $>$ Master's thesis $>$ MSc thesis guidelines},
label={Thesis guide},
langid={english}
}

@manual{pubadvice2009,
title={Practical advice for writing publications},
author={Salminen, E.},
organization={Tampere University of Technology},
location={Tampere},
year={2012},
url={http://www.cs.tut.fi/~ege/Misc/salminen_figures_styles_v14.pdf},
note={Course material for TKT-9617 Scientific Publishing},
langid={english}
}

@manual{matohje2009,
title={Matemaattisen tekstin kirjoittaminen},
author={Ruohonen, Keijo},
organization={Tampereen teknillinen yliopisto},
location={Tampere},
year={2009},
url={http://math.tut.fi/~ruohonen/D-tyo-ohje.pdf},
langid={finnish}
}

@manual{notsoshort,
title={The Not So Short Introduction to {\LaTeXe} -- Or {\LaTeXe} in 157 minutes},
author={Oetiker, T. and Partl, H. and Hyna, I. and Schlegl, E.},
year={2018},
version={6.2},
url={http://ctan.org/tex-archive/info/lshort/english},
langid={english}
}

@manual{listings,
title={Listings -- Typeset source code listings using \LaTeX},
author={Heinz, C. and Moses, B. and Hoffmann, J.},
year={2018},
version={1.7},
url={http://www.ctan.org/pkg/listings},
langid={english}
}

@manual{tikz,
title={The Ti\emph{k}Z and PGF Packages},
author={Tantau, Till},
year={2015},
version={3.0.1a},
url={http://ctan.org/pkg/pgf},
langid={english}
}

@manual{pgfplots,
title={Manual for Package pgfplots},
author={Feuersänger, Christian},
year={2018},
version={1.16},
url={http://ctan.org/pkg/pgfplots},
langid={english}
}

@manual{siunitx,
title={siunitx -- A comprehensive (SI) units package},
author={Wright, Joseph},
year={2018},
version={2.7s},
url={http://ctan.org/pkg/siunitx},
langid={english}
}

@manual{mhchem,
title={The mhchem Bundle},
author={Hensel, Martin},
year={2018},
version={4.08},
url={http://ctan.org/pkg/mhchem},
langid={english}
}

@manual{chemfig,
title={chemfig},
author={Tellechea, Christian},
year={2018},
version={1.33},
url={http://ctan.org/pkg/chemfig},
langid={english}
}

@manual{biblatex,
title={The biblatex Package},
author={Lehman, Philipp and Kime, Philip and Wemheuer, Moritz and Boruvka, Audrey and Wright, Joseph},
year={2018},
version={3.12},
url={https://ctan.org/pkg/biblatex},
langid={english}
}

@online{bibmanagement,
title={\LaTeX/Bibliography Management},
subtitle={biblatex},
date={2018-07-12},
url={https://en.wikibooks.org/w/index.php?title=LaTeX/Bibliography_Management&stable=1},
urldate={2018-12-27},
label={\LaTeX{} WikiBook},
langid={english}
}